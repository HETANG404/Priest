import utils.create_folder
from core.spider import *
from utils.create_folder import *
import datetime

from config.settings import BASE_PATH


import os
import utils.create_folder as create_folder  # å‡è®¾ä½ æ˜¯è¿™ä¹ˆå¯¼å…¥çš„
from core.spider import *  # å‡è®¾ä½ å°è£…äº†è¿™å‡ ä¸ªå‡½æ•°
import sys

from utils.pdf_factory import images_to_pdf


def print_menu():
    print("=" * 50)
    print(" Pixiv æ’ç”»ä¸‹è½½å·¥å…· ")
    print("=" * 50)
    print("1. ä¸‹è½½æ—¥å‘¨æœˆè€Œåå…«å¤©æ¢¯ï¼Œè‡ªé€‰æ¨¡å¼")
    print("2. ä¸‹è½½æŒ‡å®šç”»å¸ˆå…¨éƒ¨ä½œå“ï¼Œéœ€è¦æŒ¨æ»´")
    print("3. ä¸‹è½½å…³æ³¨ç”¨æˆ·æœ€æ–°æ›´æ–°, æ‰“æ‰“ç‰™ç¥­")
    print("4. ä¸‹è½½å…³æ³¨ç”¨æˆ·å…¨é‡æ›´æ–°ï¼Œç«åŠ›å…¨å¼€")
    print("=" * 50)

def main():
    base_url = 'https://www.pixiv.net/'
    
    while True:
        os.system('cls' if os.name == 'nt' else 'clear')
        print_menu()
        choice = input("è¯·è¾“å…¥åŠŸèƒ½ç¼–å·ï¼ˆ 1 / 2 / 3 / 4 ï¼‰ï¼š").strip()

        if choice == '1':
            valid_modes = {"daily", "weekly", "monthly", "daily_r18", "weekly_r18", "r18g"}
            print("\næ”¯æŒæ¨¡å¼ï¼šdaily / weekly / monthly / weekly_r18 / daily_r18 / r18g")
            
            mode = input("è¯·è¾“å…¥ä¸‹è½½æ¨¡å¼ï¼š").strip()

            if mode == "r18g":
                confirm = input("â— è¯¥æ¨¡å¼æ¶‰åŠä¸¥é‡ R18G å†…å®¹ï¼Œç¡®è®¤ç»§ç»­è¯·è¾“å…¥ yesï¼š").strip().lower()
                if confirm != "yes":
                    print("âš ï¸ å·²å–æ¶ˆä¸‹è½½ R18G æŽ’è¡Œæ¦œ")
                    input("æŒ‰å›žè½¦é”®è¿”å›žä¸»èœå•...")
                    continue

            if mode not in valid_modes:
                print("âŒ éžæ³•æ¨¡å¼ï¼Œè¯·é‡æ–°è¾“å…¥ã€‚")
                input("æŒ‰å›žè½¦é”®è¿”å›žä¸»èœå•...")
                continue

            method_name = f"create_{mode}_folder"
            method = getattr(create_folder, method_name, None)

            if not callable(method):
                print(f"âŒ æœªæ‰¾åˆ°å¯¹åº”çš„æ–‡ä»¶å¤¹åˆ›å»ºæ–¹æ³•: {method_name}")
                input("æŒ‰å›žè½¦é”®è¿”å›žä¸»èœå•...")
                continue

            path = method()

            # è®¾ç½®é»˜è®¤æœ€å¤§é¡µæ•°
            default_pages = {
                "daily": 10,
                "weekly": 10,
                "monthly": 10,
                "daily_r18": 2,
                "weekly_r18": 2,
                "r18g": 1
            }

            max_page = default_pages.get(mode, 1)

            # æç¤ºç”¨æˆ·è¾“å…¥é¡µæ•°
            print(f"ðŸ’¡ {mode} æ¨¡å¼æœ€å¤šæ”¯æŒ {max_page} é¡µï¼ˆæŒ‰å›žè½¦ä½¿ç”¨é»˜è®¤ï¼‰")
            user_page_input = input("è¯·è¾“å…¥è¦ä¸‹è½½çš„é¡µæ•°ï¼š").strip()

            if user_page_input == "":
                page = max_page
            else:
                if user_page_input.isdigit():
                    page = int(user_page_input)
                    if page > max_page or page <= 0:
                        print(f"âŒ é¡µæ•°è¶…å‡ºèŒƒå›´ï¼ˆ1 ~ {max_page}ï¼‰")
                        input("æŒ‰å›žè½¦é”®è¿”å›žä¸»èœå•...")
                        continue
                else:
                    print("âŒ é¡µæ•°å¿…é¡»æ˜¯æ­£æ•´æ•°")
                    input("æŒ‰å›žè½¦é”®è¿”å›žä¸»èœå•...")
                    continue

            # ç”Ÿæˆ PDF æ–‡ä»¶è·¯å¾„ï¼Œæ‹¼ä¸Šé¡µæ•°
            date_str = datetime.datetime.now().strftime('%Y-%m-%d')
            pdf_path = os.path.join(BASE_PATH, "pdfs", f"merged_{mode}_{page}p_{date_str}.pdf")

            for i in range(page):
                url = f"{base_url}ranking.php?mode={mode}&p={i + 1}&format=json"
                crawler_ranking(url, i, path)

            images_to_pdf(path, pdf_path)
            print(f"\nðŸ“„ PDF æ–‡ä»¶å·²ç”Ÿæˆï¼š{pdf_path}")



        elif choice == '2':
            user_id = input("\nè¯·è¾“å…¥ç”»å¸ˆçš„ç”¨æˆ·IDï¼š").strip()
            url = f"{base_url}ajax/user/{user_id}/profile/all?lang=zh"
            crawler_users(url, user_id)

        elif choice == '3':
            try:
                page = int(input("è¯·è¾“å…¥ä¸‹è½½é¡µæ•°ï¼ˆæ¯é¡µ60å¼ ï¼‰ï¼š").strip())
            except ValueError:
                print("âŒ è¾“å…¥æ— æ•ˆï¼Œå¿…é¡»æ˜¯æ•´æ•°ã€‚")
                input("æŒ‰å›žè½¦é”®è¿”å›žä¸»èœå•...")
                continue

            url = f"{base_url}ajax/follow_latest/illust?lang=zh&mode=r18"
            
            path = os.path.join(BASE_PATH, "following")

            if not os.path.exists(path):
                print(f"âŒ æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼š{path}ï¼Œè¯·æ‰‹åŠ¨åˆ›å»ºåŽå†è¿è¡Œç¨‹åºã€‚")
                sys.exit(1)  # ðŸš¨ ç›´æŽ¥é€€å‡ºç¨‹åº
            
            print("å¼€å§‹ä¸‹è½½...\n")
            for i in range(page):
                page_url = f"{url}&p={i + 1}"
                crawler_latest_following(page_url, path)

        elif choice == '4':


                
            print(crawler_following())


        else:
            print("âŒ è¾“å…¥é”™è¯¯ï¼Œè¯·è¾“å…¥ 1, 2 æˆ– 3ã€‚")
            input("æŒ‰å›žè½¦é”®è¿”å›žä¸»èœå•...")
            continue

        print("\nâœ… ä¸‹è½½ä»»åŠ¡å®Œæˆï¼")
        break

if __name__ == "__main__":
    main()