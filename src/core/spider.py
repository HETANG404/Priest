import os
import requests
import threading
from threading import Thread


# æ·»åŠ figæ–‡ä»¶å¤¹åˆ°Pythonè·¯å¾„ï¼Œä»¥ä¾¿èƒ½å¤Ÿå¯¼å…¥settingæ¨¡å—
# å‡è®¾å½“å‰è„šæœ¬ä¸figæ–‡ä»¶å¤¹åœ¨åŒä¸€ç›®å½•ä¸‹ï¼Œå¦‚æœä¸æ˜¯ï¼Œè¯·è°ƒæ•´è·¯å¾„
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from config.settings import headers
from utils.downloader import download_img,download_img_withgroup,download_img_force_check
from utils.save_info_to_mysql import save_illustration_detail_from_json
from utils.create_folder import create_users_folder


max_connections = 10  # å®šä¹‰æœ€å¤§çº¿ç¨‹æ•°,å¯æ ¹æ®ç½‘é€Ÿä¿®æ”¹
pool_sema = threading.BoundedSemaphore(max_connections)  # æˆ–ä½¿ç”¨Semaphoreæ–¹æ³•


# def crawler_ranking(url, page, path):  # https://www.pixiv.net/ranking.php?mode=monthly_r18&p=1&format=json   # https://www.pixiv.net/bookmark_new_illust
#     res = requests.get(url, headers=headers)
#     datas = res.json()["contents"]  # print(datas)
#     images_list = []
    
#     print(f"æ­£åœ¨ç»Ÿè®¡page:{page+1}ä¸‹è½½ä»»åŠ¡æ•°é‡...")

#     for data in datas:
#         image = {
#             "title": data["title"],
#             "user_name": data["user_name"],
#             "p_id": data["illust_id"],
#             "referer": f"https://www.pixiv.net/artworks/{data['illust_id']}"
#         }
#         images_list.append(image)  # print(images_list)

#     thread_list = []
#     download_count = 0  # æ·»åŠ ä¸‹è½½ä»»åŠ¡è®¡æ•°å™¨

#     for i in range(len(images_list)):
#         image_1 = images_list[i]
#         image_url = f"https://www.pixiv.net/ajax/illust/{image_1['p_id']}/pages?lang=zh"  # é€šè¿‡ä»¥ä¸‹é“¾æ¥ï¼Œè¯·æ±‚å›¾ç‰‡è¯¦æƒ…
        
#         # print({image_1['p_id']})
#         detail_res = requests.get(image_url, headers=headers)
#         detail_data = detail_res.json()["body"]
#         save_illustration_detail_from_json(detail_data)

#         image_data = requests.get(image_url, headers=headers).json()["body"]  # æ•°æ®ä¿å­˜åœ¨bodyå­—æ®µ        print(image_data)
        
#         for b in image_data:  # thumb_mini/small/regular/original
#             download_count += 1 # æ¯åˆ›å»ºä¸€ä¸ªä¸‹è½½ä»»åŠ¡å°±è®¡æ•°åŠ 1
#             t = Thread(target=download_img_withgroup, args=(b['urls']['original'], image_1["referer"], page * 50 + i + 1, path),
#                        name=image_1['p_id'])
#             thread_list.append(t)

#     print(f"æ€»å…±æœ‰ {download_count} ä¸ªä¸‹è½½ä»»åŠ¡")  # è¾“å‡ºæ€»ä»»åŠ¡æ•°
#     print("å¼€å§‹ä¸‹è½½...")

#     for t in thread_list:
#         t.start()  # è°ƒç”¨start()æ–¹æ³•ï¼Œå¼€å§‹æ‰§è¡Œ

#     for t in thread_list:
#         t.join()  # å­çº¿ç¨‹è°ƒç”¨join()æ–¹æ³•ï¼Œä½¿ä¸»çº¿ç¨‹ç­‰å¾…å­çº¿ç¨‹è¿è¡Œå®Œæ¯•ä¹‹åæ‰é€€å‡º
#     print(f"page:{page+1} ä¸‹è½½å®Œæˆ")

# def crawler_users(url, path):  # https://www.pixiv.net/ajax/user/23945843/profile/all?lang=zh
#     res = requests.get(url, headers=headers)
#     datas = res.json()["body"]  # print(datas["illusts"])

#     images_list = list(datas["illusts"].keys())  # print(images_list)

#     for i in range(len(images_list)):
#         image_1 = images_list[i]
#         Referer_ = f"https://www.pixiv.net/artworks/{image_1}"
#         image_url = f"https://www.pixiv.net/ajax/illust/{image_1}/pages?lang=zh"  # é€šè¿‡ä»¥ä¸‹é“¾æ¥ï¼Œè¯·æ±‚å›¾ç‰‡è¯¦æƒ…
#         image_data = requests.get(image_url, headers=headers).json()["body"]  # æ•°æ®ä¿å­˜åœ¨bodyå­—æ®µ        print(image_data)
#         for b in image_data:  # thumb_mini/small/regular/original
#             t = Thread(target=download_img, args=(b['urls']['original'], Referer_, path),
#                        name=image_1)
#             thread_list.append(t)

#     for t in thread_list:
#         t.start()  # è°ƒç”¨start()æ–¹æ³•ï¼Œå¼€å§‹æ‰§è¡Œ

#     for t in thread_list:
#         t.join()  # å­çº¿ç¨‹è°ƒç”¨join()æ–¹æ³•ï¼Œä½¿ä¸»çº¿ç¨‹ç­‰å¾…å­çº¿ç¨‹è¿è¡Œå®Œæ¯•ä¹‹åæ‰é€€å‡º

def crawler_ranking(url, page, path):
    print(f"ğŸ“„ æ­£åœ¨ç»Ÿè®¡ page {page+1} çš„ä¸‹è½½ä»»åŠ¡...")

    try:
        res = requests.get(url, headers=headers)
        res.raise_for_status()
        datas = res.json().get("contents", [])
    except Exception as e:
        print(f"âŒ è·å–æ’è¡Œæ¦œæ•°æ®å¤±è´¥ï¼š{e}")
        return

    images_list = []
    for data in datas:
        images_list.append({
            "title": data["title"],
            "user_name": data["user_name"],
            "p_id": data["illust_id"],
            "referer": f"https://www.pixiv.net/artworks/{data['illust_id']}"
        })

    thread_list = []
    download_count = 0

    for idx, image in enumerate(images_list):
        illust_id = image["p_id"]

        # è·å–ä½œå“å›¾ç‰‡ï¼ˆåŒ…æ‹¬å¤šé¡µï¼‰
        image_pages_url = f"https://www.pixiv.net/ajax/illust/{illust_id}/pages?lang=zh"
        try:
            image_data = requests.get(image_pages_url, headers=headers).json().get("body", [])
        except Exception as e:
            print(f"âŒ è·å–æ’ç”» {illust_id} å›¾ç‰‡é¡µå¤±è´¥ï¼š{e}")
            continue

        for b in image_data:
            download_count += 1
            t = Thread(
                target=download_img_withgroup,
                args=(b['urls']['original'], image["referer"], page * 50 + idx + 1, path),
                name=str(illust_id)
            )
            thread_list.append(t)

    print(f"ğŸ§® æ€»å…±åˆ›å»ºäº† {download_count} ä¸ªä¸‹è½½ä»»åŠ¡")
    print("ğŸš€ å¼€å§‹ä¸‹è½½...")

    for t in thread_list:
        t.start()

    for t in thread_list:
        t.join()

    print(f"âœ… ç¬¬ {page+1} é¡µä¸‹è½½å®Œæˆ")



def crawler_users(url, user_id):  # https://www.pixiv.net/ajax/user/23945843/profile/all?lang=zh
    res = requests.get(url, headers=headers)
    datas = res.json()["body"]  # print(datas["illusts"])

    images_list = list(datas["illusts"].keys())  # print(images_list)

    thread_list = []

    print(f"åˆ›å»º {images_list.__len__} ä¸ªå›¾ç‰‡ä¸‹è½½ä»»åŠ¡")
    #è·å–user_nameé€šè¿‡id
    info_url = f"https://www.pixiv.net/ajax/user/{user_id}?lang=zh"
    res = requests.get(info_url, headers=headers)
    user_name = res.json()["body"]["name"]
    folder_name = user_id + user_name

    path = create_users_folder(folder_name)

    print(f"å¼€å§‹ä¸‹è½½...")

    for i in range(len(images_list)):
        image_1 = images_list[i]
        Referer_ = f"https://www.pixiv.net/artworks/{image_1}"
        image_url = f"https://www.pixiv.net/ajax/illust/{image_1}/pages?lang=zh"  # é€šè¿‡ä»¥ä¸‹é“¾æ¥ï¼Œè¯·æ±‚å›¾ç‰‡è¯¦æƒ…
        image_data = requests.get(image_url, headers=headers).json()["body"]  # æ•°æ®ä¿å­˜åœ¨bodyå­—æ®µ        print(image_data)
        for b in image_data:  # thumb_mini/small/regular/original
            t = Thread(target=download_img, args=(b['urls']['original'], Referer_, path),
                       name=image_1)
            thread_list.append(t)

    for t in thread_list:
        t.start()  # è°ƒç”¨start()æ–¹æ³•ï¼Œå¼€å§‹æ‰§è¡Œ

    for t in thread_list:
        t.join()  # å­çº¿ç¨‹è°ƒç”¨join()æ–¹æ³•ï¼Œä½¿ä¸»çº¿ç¨‹ç­‰å¾…å­çº¿ç¨‹è¿è¡Œå®Œæ¯•ä¹‹åæ‰é€€å‡º



def crawler_latest_following(url, path):#  https://www.pixiv.net/ajax/follow_latest/illust?p=1&mode=r18&lang=zh
    res = requests.get(url, headers=headers)
    datas = res.json()["body"]  # print(datas["illusts"])

    # â›” åˆ¤æ–­æ˜¯å¦è¿˜æœ‰æ’ç”»
    if not datas.get("page") or not datas["page"].get("ids"):
        print("ğŸ“­ æ²¡æœ‰æ›´å¤šæ’ç”»äº†ï¼Œæå‰ç»“æŸåˆ†é¡µ")
        return False  # è¿”å› False è¡¨ç¤ºç»“æŸ

    images_list = datas["page"]["ids"]  # print(images_list, len(images_list))

    thread_list = []



    for i in range(len(images_list)):
        image_1 = images_list[i]
        Referer_ = f"https://www.pixiv.net/artworks/{image_1}"
        image_url = f"https://www.pixiv.net/ajax/illust/{image_1}/pages?lang=zh"  # é€šè¿‡ä»¥ä¸‹é“¾æ¥ï¼Œè¯·æ±‚å›¾ç‰‡è¯¦æƒ…
        image_data = requests.get(image_url, headers=headers).json()["body"]  # æ•°æ®ä¿å­˜åœ¨bodyå­—æ®µ        print(image_data)
        for b in image_data:  # thumb_mini/small/regular/original
            t = Thread(target=download_img_force_check, args=(b['urls']['original'], Referer_, path),
                       name=image_1)
            thread_list.append(t)

    for t in thread_list:
        t.start()  # è°ƒç”¨start()æ–¹æ³•ï¼Œå¼€å§‹æ‰§è¡Œ

    for t in thread_list:
        t.join()  # å­çº¿ç¨‹è°ƒç”¨join()æ–¹æ³•ï¼Œä½¿ä¸»çº¿ç¨‹ç­‰å¾…å­çº¿ç¨‹è¿è¡Œå®Œæ¯•ä¹‹åæ‰é€€å‡º



def crawler_following():
    return "è¿˜åœ¨å¼€å‘ï¼Œå…ˆåˆ«æ€¥"
    
