import os
import requests
import threading
from threading import Thread


# æ·»åŠ figæ–‡ä»¶å¤¹åˆ°Pythonè·¯å¾„ï¼Œä»¥ä¾¿èƒ½å¤Ÿå¯¼å…¥settingæ¨¡å—
# å‡è®¾å½“å‰è„šæœ¬ä¸figæ–‡ä»¶å¤¹åœ¨åŒä¸€ç›®å½•ä¸‹ï¼Œå¦‚æœä¸æ˜¯ï¼Œè¯·è°ƒæ•´è·¯å¾„
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from config.settings import headers
from utils.downloader import download_img,download_img_withgroup,download_img_force_check
from utils.save_info_to_mysql import save_illustration_detail_from_json
from utils.create_folder import create_users_folder


max_connections = 10  # å®šä¹‰æœ€å¤§çº¿ç¨‹æ•°,å¯æ ¹æ®ç½‘é€Ÿä¿®æ”¹
pool_sema = threading.BoundedSemaphore(max_connections)  # æˆ–ä½¿ç”¨Semaphoreæ–¹æ³•

def wander_crawler_ranking(url, page, path):
    print(f"ğŸ“„ æ­£åœ¨ç»Ÿè®¡ page {page+1} çš„ä¸‹è½½ä»»åŠ¡...")

    # æ·»åŠ  content=illust å‚æ•°ä»¥ä»…è·å–æ’ç”»ä½œå“
    url = f"{url}&content=illust"

    try:
        res = requests.get(url, headers=headers)
        res.raise_for_status()
        datas = res.json().get("contents", [])
    except Exception as e:
        print(f"âŒ è·å–æ’è¡Œæ¦œæ•°æ®å¤±è´¥ï¼š{e}")
        return

    images_list = []
    for data in datas:
        images_list.append({
            "title": data["title"],
            "user_name": data["user_name"],
            "p_id": data["illust_id"],
            "referer": f"https://www.pixiv.net/artworks/{data['illust_id']}"
        })

    thread_list = []
    download_count = 0

    for idx, image in enumerate(images_list):
        illust_id = image["p_id"]

        # è·å–ä½œå“å›¾ç‰‡ï¼ˆåŒ…æ‹¬å¤šé¡µï¼‰
        image_pages_url = f"https://www.pixiv.net/ajax/illust/{illust_id}/pages?lang=zh"
        try:
            image_data = requests.get(image_pages_url, headers=headers).json().get("body", [])
        except Exception as e:
            print(f"âŒ è·å–æ’ç”» {illust_id} å›¾ç‰‡é¡µå¤±è´¥ï¼š{e}")
            continue

        # ğŸ”§ å¦‚æœè¯¥ä½œå“æœ‰5é¡µåŠä»¥ä¸Šï¼Œè·³è¿‡æ•´ä¸ªä½œå“
        if len(image_data) >= 3:
            print(f"âš ï¸ æ’ç”» {illust_id} æœ‰ {len(image_data)} é¡µï¼Œå·²è·³è¿‡")
            continue

        for b in image_data:
            download_count += 1
            t = Thread(
                target=download_img_withgroup,
                args=(b['urls']['original'], image["referer"], page * 50 + idx + 1, path),
                name=str(illust_id)
            )
            thread_list.append(t)

    print(f"ğŸ§® æ€»å…±åˆ›å»ºäº† {download_count} ä¸ªä¸‹è½½ä»»åŠ¡")
    print("ğŸš€ å¼€å§‹ä¸‹è½½...")

    for t in thread_list:
        t.start()

    for t in thread_list:
        t.join()

    print(f"âœ… ç¬¬ {page+1} é¡µä¸‹è½½å®Œæˆ")